# -*- coding: utf-8 -*-
"""gen.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SxDfILh3N7h4jSdnBJdXSEChFrfNYmps
"""


import os
import json
import numpy as np
import pandas as pd
from pandas.io.json import json_normalize
import matplotlib.pyplot as plt
import seaborn as sns
color = sns.color_palette()

# %matplotlib inline

from plotly import tools
import plotly.offline as py
py.init_notebook_mode(connected=True)
import plotly.graph_objs as go

from sklearn import model_selection, preprocessing, metrics
import lightgbm as lgb

pd.options.mode.chained_assignment = None
pd.options.display.max_columns = 999

train_df = pd.read_csv("train.csv")
test_df = pd.read_csv("test_QoiMO9B.csv")
print("Number of rows and columns in train set : ",train_df.shape)
print("Number of rows and columns in test set : ",test_df.shape)

#train_df.head()

#target_col = "num_orders"

#plt.figure(figsize=(8,6))
#plt.scatter(range(train_df.shape[0]), np.sort(train_df[target_col].values))
#plt.xlabel('index', fontsize=12)
#plt.ylabel('number of orders', fontsize=12)
#plt.show()

#target_col = "num_orders"

#plt.figure(figsize=(8,6))
#plt.scatter(range(sub.shape[0]), np.sort(sub[target_col].values))
#plt.xlabel('index', fontsize=12)
#plt.ylabel('number of orders', fontsize=12)
#plt.show()

#plt.figure(figsize=(12,8))
#sns.distplot(train_df[target_col].values, bins=50, kde=False, color="red")
#plt.title("Histogram of number of orders")
#plt.xlabel('number of orders', fontsize=12)
#plt.show()

cnt_srs = train_df['week'].value_counts()
#cnt_srs = cnt_srs.sort_index()
#plt.figure(figsize=(14,6))
#sns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color='green')
#plt.xticks(rotation='vertical')
#plt.xlabel('week', fontsize=12)
#plt.ylabel('weekly dist', fontsize=12)
#plt.title("week count in train set")
#plt.show()

#cnt_srs = test_df['week'].value_counts()
#cnt_srs = cnt_srs.sort_index()
#plt.figure(figsize=(14,6))
#sns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color='green')
#plt.xticks(rotation='vertical')
#plt.xlabel('week', fontsize=12)
#plt.ylabel('weekly dist', fontsize=12)
#plt.title("week count in test set")
#plt.show()

## feature 1
#plt.figure(figsize=(8,4))
#sns.violinplot(x="week", y=target_col, data=train_df)
#plt.xticks(rotation='vertical')
#plt.xlabel('week', fontsize=12)
#plt.ylabel('number of orders', fontsize=12)
#plt.title("week distribution")
#plt.show()

## feature 2
#plt.figure(figsize=(8,4))
#sns.violinplot(x="checkout_price", y=target_col, data=train_df)
#plt.xticks(rotation='vertical')
#plt.xlabel('checkout_price', fontsize=12)
#plt.ylabel('number of orders', fontsize=12)
#plt.title("checkout_price distribution")
#plt.show()

## feature 3
#plt.figure(figsize=(8,4))
#sns.violinplot(x="base_price", y=target_col, data=train_df)
#plt.xticks(rotation='vertical')
#plt.xlabel('base_price', fontsize=12)
#plt.ylabel('number of orders', fontsize=12)
#plt.title("base_price distribution")
#plt.show()

## feature 1
#plt.figure(figsize=(8,4))
#sns.violinplot(x="emailer_for_promotion", y=target_col, data=train_df)
#plt.xticks(rotation='vertical')
#plt.xlabel('emailer_for_promotion', fontsize=12)
#plt.ylabel('number of orders', fontsize=12)
#plt.title("emailer_for_promotion distribution")
#plt.show()

## feature 2
#plt.figure(figsize=(8,4))
#sns.violinplot(x="homepage_featured", y=target_col, data=train_df)
#plt.xticks(rotation='vertical')
#plt.xlabel('homepage_featured', fontsize=12)
#plt.ylabel('number of orders', fontsize=12)
#plt.title("homepage_featured distribution")
#plt.show()

ful_df = pd.read_csv("fulfilment_center_info.csv")
#ful_df.head()

# gdf = ful_df.groupby("center_id",as_index=False)
# gdf = gdf["op_area"].size().reset_index()
# gdf = ["center_id", "op_area"]
train_df = pd.merge(train_df, ful_df, on="center_id", how="left")
test_df = pd.merge(test_df, ful_df, on="center_id", how="left")

meal_df = pd.read_csv("meal_info.csv")
#meal_df.head()

train_df = pd.merge(train_df, meal_df, on="meal_id", how="left")
test_df = pd.merge(test_df, meal_df, on="meal_id", how="left")

#train_df.head()

df = pd.concat([train_df,test_df])
df = df.drop(["num_orders"],axis=1)

#from sklearn.preprocessing import StandardScaler, RobustScaler

## RobustScaler is less prone to outliers.

#std_scaler = StandardScaler()
#rob_scaler = RobustScaler()

#df['checkout_price'] = rob_scaler.fit_transform(df['checkout_price'].values.reshape(-1,1))
#df['base_price'] = rob_scaler.fit_transform(df['base_price'].values.reshape(-1,1))
#df['']

#train_df.corr()

df["center_type"] = df["center_type"].astype('category')
df["category"] = df["category"].astype('category')
df["center_id"] = df["center_id"].astype('category')
df["meal_id"] = df["meal_id"].astype('category')
df["region_code"] = df["region_code"].astype('category')
df["cuisine"] = df["cuisine"].astype('category')
df["city_code"] = df["city_code"].astype('category')




df["center_type"] = df["center_type"].cat.codes
df["category"] = df["category"].cat.codes
df["cuisine"] = df["cuisine"].cat.codes
df["center_id"] = df["center_id"].cat.codes
df["meal_id"] = df["meal_id"].cat.codes
df["region_code"] = df["region_code"].cat.codes
df["city_code"] = df["city_code"].cat.codes

#df.region_code.value_counts()

from sklearn.preprocessing import StandardScaler, RobustScaler

# RobustScaler is less prone to outliers.

std_scaler = StandardScaler()
rob_scaler = RobustScaler()

df['checkout_price'] = rob_scaler.fit_transform(df['checkout_price'].values.reshape(-1,1))
df['base_price'] = rob_scaler.fit_transform(df['base_price'].values.reshape(-1,1))
df['center_id'] = rob_scaler.fit_transform(df['center_id'].values.reshape(-1,1))
df['city_code'] = rob_scaler.fit_transform(df['city_code'].values.reshape(-1,1))
df['meal_id'] = rob_scaler.fit_transform(df['meal_id'].values.reshape(-1,1))
# df['week'] = rob_scaler.fit_transform(df['week'].values.reshape(-1,1))

df.head()

def rmsle(predicted,real):
        sum=0.0
        for x in range(len(predicted)):
            p = np.log(predicted[x]+1)
            r = np.log(real[x]+1)
            sum = sum + (p - r)**2
        return (sum/len(predicted))**0.5


def rmsle_score(preds, dtrain):
        labels = dtrain.get_label()
        return 'RMSLE', rmsle(labels, preds)

#len_train = train_df.shape[0]

#train = df[:len_train]
#test = df[len_train:]
#from sklearn.metrics import mean_squared_log_error
#median_num_orders = np.median(train_df['num_orders'].values)

#cols_to_use = ["base_price", "category", "center_type", "checkout_price", "cuisine", 
               #"emailer_for_promotion", "homepage_featured", "op_area","center_id","region_code","meal_id","city_code"]
#def rmsle(y_true, y_pred):
    #assert len(y_true) == len(y_pred)
    #return np.sqrt(np.mean(np.power(np.log1p(y_true + 1) - np.log1p(y_pred + 1), 2)))

#def run_lgb(train_X, train_y, val_X, val_y, test_X):
    #params = {
        #"objective" : "regression",
        #"eval_metric" : "mean_squared_log_error",
        #"metric" : "rmse",
        #"num_leaves" : 30,
        #"min_child_weight" : 50,
        #"learning_rate" : 0.05,
        #"bagging_fraction" : 0.7,
        #"feature_fraction" : 0.7,
        #"bagging_frequency" : 5,
        #"bagging_seed" : 2018,
        #"verbosity" : -1
    #}
    
    #lgtrain = lgb.Dataset(train_X, label=train_y)
    #lgval = lgb.Dataset(val_X, label=val_y)
    #evals_result = {}
    #model = lgb.train(params, lgtrain, 20000, valid_sets=[lgval], early_stopping_rounds=100, verbose_eval=100, evals_result=evals_result)
    
    #pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)
    #return pred_test_y, model, evals_result

#train_X = train[cols_to_use]
#test_X = test[cols_to_use]
#train_y = train_df[target_col].values

#pred_test = 0
#kf = model_selection.KFold(n_splits=5, random_state=2018, shuffle=True)
#for dev_index, val_index in kf.split(train):
    #dev_X, val_X = train_X.loc[dev_index,:], train_X.loc[val_index,:]
    #dev_y, val_y = train_y[dev_index], train_y[val_index]
    
    #pred_test_tmp, model, evals_result = run_lgb(dev_X, dev_y, val_X, val_y, test_X)
    #pred_test_tmp[pred_test_tmp<0]=median_num_orders
    #pred_test += pred_test_tmp
#pred_test /= 5.

#def gini(actual, pred, cmpcol = 0, sortcol = 1):
    #assert( len(actual) == len(pred) )
    #all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)
    #all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]
    #totalLosses = all[:,0].sum()
    #giniSum = all[:,0].cumsum().sum() / totalLosses
    
    #giniSum -= (len(actual) + 1) / 2.
    #return giniSum / len(actual)

#def gini_normalized(a, p):
    #return gini(a, p) / gini(a, a)

#def gini_xgb(preds, dtrain):
    #labels = dtrain.get_label()
    #gini_score = gini_normalized(labels, preds)
    #return [('gini', gini_score)]

#from sklearn.model_selection import train_test_split
#from sklearn.model_selection import KFold
#xgb_preds = []
#K = 5
## kf = KFold(n_splits = K, random_state = 3228, shuffle = True)
#kf = model_selection.KFold(n_splits=5, random_state=2018, shuffle=True)

#for dev_index, val_index in kf.split(train):
    #train_X, valid_X = train_X.loc[dev_index,:], train_X.loc[val_index,:]
    #train_y, valid_y = train_y[dev_index], train_y[val_index]
## dev_X, val_X = train_X.loc[dev_index,:], train_X.loc[val_index,:]
##     dev_y, val_y = train_y[dev_index], train_y[val_index]
    ## params configuration also from the1owl's kernel
    ## https://www.kaggle.com/the1owl/forza-baseline
    #xgb_params = {'eta': 0.02, 'max_depth': 4, 'subsample': 0.9, 'colsample_bytree': 0.9, 'objective': 'reg:linear', 'eval_metric': 'rmse', 'seed': 99, 'silent': True}

    #d_train = xgb.DMatrix(train_X, train_y)
    #d_valid = xgb.DMatrix(valid_X, valid_y)
    #d_test = xgb.DMatrix(test_X)
    
    #watchlist = [(d_train, 'train'), (d_valid, 'valid')]
    #model = xgb.train(xgb_params, d_train, 5000,  watchlist, feval=gini_xgb, maximize=True, verbose_eval=100, early_stopping_rounds=100)
                        
    #xgb_pred = model.predict(d_test)
    #xgb_pred[xgb_pred<0] = median_num_orders
    #xgb_preds.append(list(xgb_pred))

#pred_test

#fig, ax = plt.subplots(figsize=(12,10))
#lgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)
#ax.grid(False)
#plt.title("LightGBM - Feature Importance", fontsize=15)
#plt.show()

#predictions = model.predict(test)

#sub = pd.read_csv('/content/gdrive/Colab/genpact/sample_submission_hSlSoT6.csv')
#sub["num_orders"] = pred_test
#sub.to_csv('/content/gdrive/Colab/genpact/sub_week_scale4.csv', index=False)
#sub.head()

#sub.shape

#subm = pd.DataFrame()
#subm["id"] = test["id"].values

#subm['num_orders'] = predictions
#subm.head()

#subm.to_csv('/content/gdrive/Colab/genpact/sub.csv',index=False,header=False)

#subm.shape

#test_df.shape

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
# %matplotlib inline
from subprocess import check_output
import warnings
warnings.filterwarnings('ignore')

import seaborn as sns
from sklearn.metrics import accuracy_score, classification_report, mean_squared_error
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import KFold, cross_val_score
from sklearn.model_selection import train_test_split

models = []
models.append(('LR', LogisticRegression()))
models.append(('LDA', LinearDiscriminantAnalysis()))
models.append(('QDA', QuadraticDiscriminantAnalysis()))
models.append(('KNN', KNeighborsClassifier()))
models.append(('DT', DecisionTreeClassifier()))
models.append(('RF', RandomForestClassifier(n_estimators=1000)))
models.append(('SVM', SVC()))
models.append(('NB', GaussianNB()))

len_train = train_df.shape[0]

train = df[:len_train]
test = df[len_train:]

results = []
name = []
for names, model in models:
    kfold = KFold(n_splits=10, random_state=7)
    mod_results = cross_val_score(model, train,train_df['num_orders'], cv=kfold, scoring='neg_mean_squared_log_error')
    results.append(mod_results)
    name.append(names)
    print("%s : %f" %(names, mod_results.mean()))
# boxplot algorithm comparison
fig = plt.figure()
fig.suptitle('Model Comparison')
ax = fig.add_subplot(111)
plt.boxplot(results)
ax.set_xticklabels(name)
plt.show()

